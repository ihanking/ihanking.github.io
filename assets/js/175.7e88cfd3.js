(window.webpackJsonp=window.webpackJsonp||[]).push([[175],{652:function(v,_,p){"use strict";p.r(_);var t=p(4),n=Object(t.a)({},(function(){var v=this,_=v.$createElement,p=v._self._c||_;return p("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[p("p",[p("strong",[v._v("1.男生点击率增加，女生点击率增加，总体为何减少?")])]),v._v(" "),p("p",[v._v("因为男女的点击率可能有较大差异，同时低点击率群体的占比增大。\n如原来男性20人，点击1人；女性100人，点击99人，总点击率100/120。\n现在男性100人，点击6人；女性20人，点击20人，总点击率26/120。\n即那个段子“A系中智商最低的人去读B，同时提高了A系和B系的平均智商。”")]),v._v(" "),p("p",[p("strong",[v._v("2.协方差与相关系数的区别和联系")])]),v._v(" "),p("p",[v._v("协方差：")]),v._v(" "),p("p",[v._v("协方差表示的是两个变量的总体的误差，这与只表示一个变量误差的方差不同。")]),v._v(" "),p("p",[v._v("如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。")]),v._v(" "),p("p",[v._v("如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。")]),v._v(" "),p("p",[v._v("相关系数：")]),v._v(" "),p("p",[v._v("研究变量之间线性相关程度的量，取值范围是[-1,1]。")]),v._v(" "),p("p",[v._v("相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差。")]),v._v(" "),p("p",[p("strong",[v._v("3.中心极限定理")])]),v._v(" "),p("p",[v._v("中心极限定理定义：")]),v._v(" "),p("p",[v._v("（1）任何一个样本的平均值将会约等于其所在总体的平均值。")]),v._v(" "),p("p",[v._v("（2）不管总体是什么分布，任意一个总体的样本平均值都会围绕在总体的平均值周围，并且呈正态分布。")]),v._v(" "),p("p",[v._v("中心极限定理作用：")]),v._v(" "),p("p",[v._v("（1）在没有办法得到总体全部数据的情况下，我们可以用样本来估计总体。")]),v._v(" "),p("p",[v._v("（2）根据总体的平均值和标准差，判断某个样本是否属于总体。")]),v._v(" "),p("p",[p("strong",[v._v("4.怎么向小孩子解释正态分布")])]),v._v(" "),p("p",[v._v("（随口追问了一句小孩子的智力水平，面试官说七八岁，能数数）")]),v._v(" "),p("p",[v._v("拿出小朋友班级的成绩表，每隔2分统计一下人数（因为小学一年级大家成绩很接近），画出钟形。然后说这就是正态分布，大多数的人都集中在中间，只有少数特别好和不够好\n拿出隔壁班的成绩表，让小朋友自己画画看，发现也是这样的现象\n然后拿出班级的身高表，发现也是这个样子的\n大部分人之间是没有太大差别的，只有少数人特别好和不够好，这是生活里普遍看到的现象，这就是正态分布")]),v._v(" "),p("p",[p("strong",[v._v("5.什么是聚类分析？聚类算法有哪几种？请选择一种详细描述其计算原理和步骤")])]),v._v(" "),p("p",[v._v("（1）聚类分析是一种无监督的学习方法，根据一定条件将相对同质的样本归到一个类总（俗话说人以类聚，物以群分）")]),v._v(" "),p("p",[v._v("正式一点的：聚类是对点集进行考察并按照某种距离测度将他们聚成多个“簇”的过程。聚类的目标是使得同一簇内的点之间的距离较短，而不同簇中点之间的距离较大。")]),v._v(" "),p("p",[v._v("（2）聚类方法主要有：")]),v._v(" "),p("p",[v._v("a. 层次聚类")]),v._v(" "),p("p",[v._v("层次法（hierarchical methods），这种方法对给定的数据集进行层次似的分解，直到某种条件满足为止。具体又可分为“自底向上”和“自顶向下”两种方案。")]),v._v(" "),p("p",[v._v("b. 划分聚类：（经典算法为kmeans）")]),v._v(" "),p("p",[v._v("划分法（parTITIoning methods），给定一个有N个元组或者纪录的数据集，分裂法将构造K个分组，每一个分组就代表一个聚类，K《N。而且这K个分组满足下列条件：")]),v._v(" "),p("p",[v._v("（1） 每一个分组至少包含一个数据纪录；")]),v._v(" "),p("p",[v._v("（2）每一个数据纪录属于且仅属于一个分组（注意：这个要求在某些模糊聚类算法中可以放宽）；")]),v._v(" "),p("p",[v._v("对于给定的K，算法首先给出一个初始的分组方法，以后通过反复迭代的方法改变分组，使得每一次改进之后的分组方案都较前一次好，而所谓好的标准就是：同一分组中的记录越近越好，而不同分组中的纪录越远越好。")]),v._v(" "),p("p",[v._v("c. 密度聚类")]),v._v(" "),p("p",[v._v("基于密度的方法（density-based methods），基于密度的方法与其它方法的一个根本区别是：它不是基于各种各样的距离的，而是基于密度的。这样就能克服基于距离的算法只能发现“类圆形”的聚类的缺点。")]),v._v(" "),p("p",[v._v("经典算法：DBSCAN:DBSCAN算法是一种典型的基于密度的聚类算法，该算法采用空间索引技术来搜索对象的邻域，引入了“核心对象”和“密度可达”等概念，从核心对象出发，把所有密度可达的对象组成一个簇。")]),v._v(" "),p("p",[v._v("这个方法的指导思想：只要一个区域中的点的密度大过某个阈值，就把它加到与之相近的聚类中去。")]),v._v(" "),p("p",[v._v("d. 网格聚类")]),v._v(" "),p("p",[v._v("基于网格的方法（grid-based methods），这种方法首先将数据空间划分成为有限个单元（cell）的网格结构，所有的处理都是以单个的单元为对象的。这么处理的一个突出的优点就是处理速度很快，通常这是与目标数据库中记录的个数无关的，它只与把数据空间分为多少个单元有关。")]),v._v(" "),p("p",[v._v("经典算法：STING：利用网格单元保存数据统计信息，从而实现多分辨率的聚类")]),v._v(" "),p("p",[v._v("e. 模型聚类：高斯混合模型")]),v._v(" "),p("p",[v._v("基于模型的方法（model-based methods），基于模型的方法给每一个聚类假定一个模型，然后去寻找能够很好的满足这个模型的数据集。这样一个模型可能是数据点在空间中的密度分布函数或者其它。它的一个潜在的假定就是：目标数据集是由一系列的概率分布所决定的。")]),v._v(" "),p("p",[v._v("（3）k-means比较好介绍，选k个点开始作为聚类中心，然后剩下的点根据距离划分到类中；找到新的类中心；重新分配点；迭代直到达到收敛条件或者迭代次数。 优点是快；缺点是要先指定k，同时对异常值很敏感。")]),v._v(" "),p("p",[p("strong",[v._v("6.线性回归和逻辑回归的区别")])]),v._v(" "),p("p",[v._v("线性回归针对的目标变量是区间型的")]),v._v(" "),p("p",[v._v("逻辑回归针对的目标变量是类别型的")]),v._v(" "),p("p",[v._v("线性回归模型的目标变量和自变量之间的关系假设是线性相关的")]),v._v(" "),p("p",[v._v("逻辑回归模型中的目标变量和自变量是非线性的")]),v._v(" "),p("p",[v._v("线性回归中通常会用假设，对应于自变量x的某个值，目标变量y的观察值是服从正太分布的")]),v._v(" "),p("p",[v._v("逻辑回归中目标变量y是服从二项分布0和1或者多项分布的")]),v._v(" "),p("p",[v._v("逻辑回归中不存在线性回归中常见的残差")]),v._v(" "),p("p",[v._v("参数估值上，线性回归采用最小平方法，逻辑回归采用最大似染法。")]),v._v(" "),p("p",[p("strong",[v._v("7.为什么说朴素贝叶斯是“朴素”的？")])]),v._v(" "),p("p",[v._v("朴素贝叶斯是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，是因为它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。")]),v._v(" "),p("p",[p("strong",[v._v("8.K-Means 和 KNN 算法的区别是什么？")])]),v._v(" "),p("p",[v._v("首先，这两个算法解决的是数据挖掘中的两类问题。K-Means 是聚类算法，KNN 是分类算法。")]),v._v(" "),p("p",[v._v("其次，这两个算法分别是两种不同的学习方式。K-Means 是非监督学习，也就是不需要事先给出分类标签，而 KNN 是有监督学习，需要我们给出训练数据的分类标识。")]),v._v(" "),p("p",[v._v("最后，K 值的含义不同。K-Means 中的 K 值代表 K 类。KNN 中的 K 值代表 K 个最接近的邻居。")]),v._v(" "),p("p",[p("strong",[v._v("9.统计的ab test，t test，如果统计显著但是实际发现不显著是什么原因，怎么验证？")])]),v._v(" "),p("p",[v._v("所谓统计意义上的显著性是指在不同总体之间的差异比较研究中,由于各个总体存在内在的变异性,而只有在当两个总体之间的差异超过单个总体内部这类变异性时,它们间的差异才具有统计上的显著性。")]),v._v(" "),p("p",[v._v("否则,当单个总体的内在变异性超过两个总体之间的差异性时,我们就称两个总体之间的差异不具有统计意义上的显著性。")]),v._v(" "),p("p",[p("strong",[v._v("10. 逻辑斯蒂回归和线性回归的区别")])]),v._v(" "),p("p",[v._v("逻辑斯蒂回归的预测值是两元的，0或1，而线性回归的预测值是连续的")]),v._v(" "),p("p",[p("strong",[v._v("11.数据建模")])]),v._v(" "),p("p",[v._v("（1）描述logistic回归与线性回归的区别")]),v._v(" "),p("p",[v._v("（2）简述有监督学习和无监督学习的区别与联系")]),v._v(" "),p("p",[v._v("（3）请举出几个分类模型的评估指标，请举出几个回归模型的评估指标")]),v._v(" "),p("p",[v._v("（4）简述工作的工作或者学校项目中，统计模型建模的基本流程（可结合分析项目说明)")]),v._v(" "),p("p",[p("strong",[v._v("12. PCA为什么要中心化？PCA的主成分是什么？")])]),v._v(" "),p("p",[p("strong",[v._v("13.极大似然估计")])]),v._v(" "),p("p",[v._v("利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。")]),v._v(" "),p("p",[p("strong",[v._v("14.置信度与置信区间是什么意思？")])]),v._v(" "),p("p",[v._v("置信区间是我们所计算出的变量存在的范围，置信水平就是我们对于这个数值存在于我们计算出的这个范围的可信程度。")]),v._v(" "),p("p",[v._v("举例来讲，如果我们有95%的把握，让真正的数值在我们所计算的范围里，那么在这里，95%是置信水平，而计算出的范围，就是置信区间。")]),v._v(" "),p("p",[v._v("如果置信度为95%， 则抽取100个样本来估计总体的均值，由100个样本所构造的100个区间中，约有95个区间包含总体均值。")]),v._v(" "),p("p",[p("strong",[v._v("15.说出两种不同的参数估计方法，并详细介绍其中一种估计方法，对某未知参数，如何比较两个不同估计量的优劣。")])]),v._v(" "),p("p",[v._v("极大似然估计，最小二乘估计（最小均方误差），矩估计（用样本 k 阶矩代替总体的 k 阶矩）。")]),v._v(" "),p("p",[v._v("矩估计法（也称数字特征法）：")]),v._v(" "),p("p",[v._v("直观意义比较明显，但要求总体 k 阶矩存在。")]),v._v(" "),p("p",[v._v("缺点是不唯一，此时尽量使用样本低阶矩。")]),v._v(" "),p("p",[v._v("观测值受异常值影响较大，不够稳健，实际中避免使用样本高阶矩。")]),v._v(" "),p("p",[v._v("估计值可能不落在参数空间")]),v._v(" "),p("p",[v._v("极大似然估计法：")]),v._v(" "),p("p",[v._v("具有一些理论上的优点（不变性、相合性、渐近正态性）")]),v._v(" "),p("p",[v._v("缺点是如果似然函数不可微，没有一般的求解法则。")]),v._v(" "),p("p",[p("strong",[v._v("16.详细介绍一种非参数统计的方法，并叙述非参数统计的优缺点")])]),v._v(" "),p("p",[v._v("非参数统计：对总体的分布不作假设或仅作非常一般性假设条件下的统计方法。")]),v._v(" "),p("p",[v._v("机器学习：决策树，随机森林，SVM；")]),v._v(" "),p("p",[v._v("假设检验：符号，符号秩，秩和检验")]),v._v(" "),p("p",[v._v("优点：")]),v._v(" "),p("p",[v._v("非参数统计方法要求的假定条件比较少，因而它的适用范围比较广泛。")]),v._v(" "),p("p",[v._v("多数非参数统计方法要求的思想与运算比较简单，可以迅速完成计算取得结果。")]),v._v(" "),p("p",[v._v("缺点：")]),v._v(" "),p("p",[v._v("由于方法简单，用的计量水准较低，因此，如果能与参数统计方法同时使用时，就不如参数统计方法敏感。")]),v._v(" "),p("p",[v._v("若为追求简单而使用非参数统计方法，其检验功效就要差些。这就是说，在给定的显著性水平下进行检验时，非参数统计方法与参数统计方法相比，第Ⅱ类错误的概率β要大些。")]),v._v(" "),p("p",[v._v("对于大样本，如不采用适当的近似，计算可能变得十分复杂。")]),v._v(" "),p("p",[p("strong",[v._v("17. ① 依概率收敛、② 概率1收敛（几乎处处收敛）的定义分别是什么？二者有什么关系？")])]),v._v(" "),p("p",[p("strong",[v._v("18.谈谈你对假设检验中，显著性水平，第一类错误，第二类错误，p值，真实水平的理解")])]),v._v(" "),p("p",[v._v("假设检验：是根据样本来推断总体的一些给定陈述是否成立的过程")]),v._v(" "),p("p",[v._v("第一类错误(type I error)：拒绝了正确零假设")]),v._v(" "),p("p",[v._v("第二类错误(type II error)：接受了不正确零假设")]),v._v(" "),p("p",[v._v("显著性水平(level of significance) : 拒绝了正确零假设的最大概率（事先给定）")]),v._v(" "),p("p",[v._v("检验功效(power) : 拒绝了不正确零假设概率")]),v._v(" "),p("p",[v._v("检验的p-值：根据样本，在原假设成立的前提下，出现与样本相同或者更极端的情况的概率")])])}),[],!1,null,null,null);_.default=n.exports}}]);