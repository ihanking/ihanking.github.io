---
title: 网剧《隐秘的角落》数据分析
date: 2020-09-27
draft: false
tags: ["requests","pandas"]
categories: ["分析实战"]
---

最近网剧《隐秘的角落》火热，豆瓣评分挺高

![隐秘的角落评分](/images/202009/22/隐秘的角落评分.jpg)

接下来就网友对这部剧的评论和看法，进行简单的分析

## 数据获取

首先从豆瓣评论页面后台入口抓取网友的评论信息

隐秘的角落：`https://movie.douban.com/subject/33404425/comments?start=20&limit=20&sort=new_score&status=P&comments_only=1`

## 分词

数据抓取下来后，对评论信息进行分词处理

```python
import pandas as pd

data=pd.read_excel("data.xlsx")

txt = data['Content'].str.cat(sep='。')

# 文本预处理  去除一些无用的字符   只提取出中文出来
new_data = re.findall('[\u4e00-\u9fa5]+', txt, re.S)  # 只要字符串中的中文
new_data = " ".join(new_data)

# 文本分词--精确模式
seg_list_exact = jieba.cut(new_data, cut_all=False)

result_list = []
with open('stop_words.txt', encoding='utf-8') as f:
    con = f.readlines()
    stop_words = set()
    for i in con:
        i = i.replace("\n", "")   # 去掉读取每一行数据的\n
        stop_words.add(i)

for word in seg_list_exact:
    # 设置停用词并去除单个词
    if word not in stop_words and len(word) > 1:
        result_list.append(word)
# print(result_list)

# 筛选后统计
word_counts = collections.Counter(result_list)
# 获取前100最高频的词
word_counts_top100 = word_counts.most_common(100)
# 打印出来看看统计的词频
# print(word_counts_top100)
```

## 词云图

最后进行词云图展示

![隐秘的角落](/images/202009/22/隐秘的角落词云图.jpg)