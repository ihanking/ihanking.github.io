---
title: 爬虫实战|爬取某咖啡巨头门店数据
date: 2020-09-10
draft: false
tags: ["requests"]
categories: ["爬虫系列"]
---

## 项目说明

通过python代码抓取星*克[官网](通过python代码抓取星*克[官网](https://www.starbucks.com.cn/stores/?features=&bounds=94.430274%2C27.164508%2C138.727149%2C44.424179)门店数据

## 爬取过程

首先查看网页结构可知，数据以json格式存储

要拿到请求参数才能访问数据，这里的参数主要是`radius`，更改数字大小即可

为了获取更多数据，这次将数字调整为100000

### 提取字段

返回json数据后需要提取数据

```python
id=i['id']
name=i['name']
city=i['address']['city']
postalCode=i['address']['postalCode']
addr1=i['address']['streetAddressLine1']
i['address'].setdefault("streetAddressLine2")
i['address'].setdefault("streetAddressLine3")
addr2=i['address']['streetAddressLine2']
addr3=i['address']['streetAddressLine3']
if addr2 and addr3:
    address=addr1+addr3+addr2
elif addr2:
    address=addr1+addr2
elif addr3:
    address=addr1+addr3
else:
    address=addr1
lat=i['coordinates']['latitude']
lng=i['coordinates']['longitude']
```

其中咖啡类型在json字段有
```
"OG","DL","MIC","MOP","NCB","RC","BE","PO","SBB"
```

所以这里需要手动以字典形式遍历
```python
fea={"OG":"Origami便携式滴滤咖啡","DL":"专星送","MIC":"星巴克冰淇淋系列","MOP":"啡快","NCB":"气致冷萃咖啡","RC":"星巴克臻选","BE":"星巴克臻选特调饮品","PO":"手冲咖啡","SBB":""}

features=[]
if i['features']:
    for f in i['features']:
        features.append(self.fea[f])
```

## 完整代码

```python
import math
import time
import json
import shutil
import random
import requests
import re,os,csv
import traceback
import pandas as pd
from loguru import logger
from parsel import Selector
from dataclasses import dataclass
from copyheaders import headers_raw_to_dict
session=requests.Session()
requests.packages.urllib3.disable_warnings()

@dataclass
class Spider(object):
    sku = os.path.splitext(os.path.basename(__file__))[0]
    r_h=b'''
    Cookie: _ga=GA1.3.1597107431.1599727489; _gid=GA1.3.458579779.1599727489; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2217477305323684-0dde62e55866c5-15336250-1296000-174773053246b1%22%2C%22first_id%22%3A%22%22%2C%22props%22%3A%7B%7D%2C%22%24device_id%22%3A%2217477305323684-0dde62e55866c5-15336250-1296000-174773053246b1%22%7D; sajssdk_2015_cross_new_user=1; _gat_UA-16990907-1=1
    Host: www.starbucks.com.cn
    User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36
    x-store-api-version: v2
    '''
    headers=headers_raw_to_dict(r_h)
    data_list = []
    base_url='https://www.starbucks.com.cn/api/stores/nearby?lat=39.907252&lon=116.385459&limit=10000&locale=ZH&features=&radius=10000000'
    keys=['id','city','name','address','postalCode','features','lat','lng']
    fea={"OG":"Origami便携式滴滤咖啡","DL":"专星送","MIC":"星巴克冰淇淋系列","MOP":"啡快","NCB":"气致冷萃咖啡","RC":"星巴克臻选","BE":"星巴克臻选特调饮品","PO":"手冲咖啡","SBB":""}

    def get_store(self,):
        try:
            resp=session.get(url=self.base_url,headers=self.headers,verify=False).json()
            for i in resp['data']:
                id=i['id']
                name=i['name']
                city=i['address']['city']
                postalCode=i['address']['postalCode']

                addr1=i['address']['streetAddressLine1']
                i['address'].setdefault("streetAddressLine2")
                i['address'].setdefault("streetAddressLine3")
                addr2=i['address']['streetAddressLine2']
                addr3=i['address']['streetAddressLine3']
                if addr2 and addr3:
                    address=addr1+addr3+addr2
                elif addr2:
                    address=addr1+addr2
                elif addr3:
                    address=addr1+addr3
                else:
                    address=addr1

                features=[]
                if i['features']:
                    for f in i['features']:
                        features.append(self.fea[f])

                lat=i['coordinates']['latitude']
                lng=i['coordinates']['longitude']
                result=id,city,name,address,postalCode,",".join(features),lat,lng
                self.data_list.append(result)
                logger.info(result)
        except Exception:
            traceback.print_exc()

    def save_store(self):
        all_data=sorted(list(set(self.data_list)),key=self.data_list.index)
        data = pd.DataFrame(all_data,columns=[h.title() for h in self.keys])
        dirname=time.strftime("%Y%m%d",time.localtime())
        os.makedirs(dirname,exist_ok=True)
        skufile='./{0}/'.format(dirname)+self.sku+dirname
        os.makedirs(skufile,exist_ok=True)
        shutil.copy(self.sku+'.py',skufile+'/'+self.sku+'.py')
        data.to_excel(skufile+'/{0}{1}.xlsx'.format(self.sku,dirname),index=False)
        logger.info("data saved successfully")

    def start(self):
        self.get_store()
        self.save_store()

if __name__=='__main__':
    Spider().start()
```