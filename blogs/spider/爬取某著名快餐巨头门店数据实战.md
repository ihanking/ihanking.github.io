---
title: "爬取某著名快餐巨头门店数据实战"
date: 2020-08-27
draft: false
tags: ["requests"]
categories: ["爬虫实战实战"]
---

### 项目说明

通过python代码抓取*德基[官网](http://www.kfc.com.cn/kfccda/storelist/index.aspx)门店数据

### 爬取过程

首先查看网页结构可知，数据以json格式存储

要拿到请求参数才能访问数据，这里的参数主要是`cname`，也就是城市信息

解析网页信息，获取城市数据
```python
def get_city(self):
        try:
            resp=session.get(url=self.base_url,headers=self.headers,verify=False).content.decode()
            sel=Selector(resp)
            all_li=sel.xpath("//ul[@class='shen_info']/li")
            for p in all_li:
                all_a=p.xpath("./div[@class='shen_city']/a")
                for c in all_a:
                    city=c.xpath("./text()").get()
        except Exception:
            traceback.print_exc()
```
这里用到`Selector`解析库来解析返回的网页内容

请求方式`post`，并以字典形式请求访问
```python
form={"cname":city,"pid":"","pageIndex": "1","pageSize": "10"}
```

出于考虑反爬机制，这里限制了访问速度
```python
time.sleep(random.random()+1)
```

至此可以返回只能部分数据，因为还有翻页数据，所以还要拿到每个城市的总页码数，并逐个遍历。
```python
page_resp=session.post(url=self.store_url,headers=self.headers,verify=False,data=form).json()
            if page_resp:
                total=page_resp['Table'][0]['rowcount']
                page_count=math.ceil(total/10)
                for page in range(1,page_count+1):
                    form['pageIndex']=str(page)
```

返回了对应页的json数据并解析
```python
store_resp=session.post(url=self.store_url,headers=self.headers,verify=False,data=form).json()
                    if store_resp:
                        for i in store_resp['Table1']:
                            result=i['rownum'],i['provinceName'],i['cityName'],i['storeName']+"餐厅",i['addressDetail'],i['pro']
                            self.data_list.append(result)
                            logger.info(result)
```

最后数据以excel形式保存到本地
```python
def save_store(self):
        all_data=sorted(list(set(self.data_list)),key=self.data_list.index)
        data = pd.DataFrame(all_data,columns=[h.title() for h in self.keys])
        dirname=time.strftime("%Y%m%d",time.localtime())
        os.makedirs(dirname,exist_ok=True)
        skufile='./{0}/'.format(dirname)+self.sku+dirname
        os.makedirs(skufile,exist_ok=True)
        shutil.copy(self.sku+'.py',skufile+'/'+self.sku+'.py')
        data.to_excel(skufile+'/{0}{1}.xlsx'.format(self.sku,dirname),index=False)
        logger.info("data saved successfully")
```

### 完整代码

```python
import math
import time
import json
import shutil
import random
import requests
import re,os,csv
import traceback
import pandas as pd
from loguru import logger
from parsel import Selector
from dataclasses import dataclass
from copyheaders import headers_raw_to_dict
session=requests.Session()
requests.packages.urllib3.disable_warnings()

@dataclass
class Spider(object):
    sku = os.path.splitext(os.path.basename(__file__))[0]
    r_h=b'''
    Cookie: route-cell=ksa; Hm_lvt_1039f1218e57655b6677f30913227148=1598507716; Hm_lpvt_1039f1218e57655b6677f30913227148=1598507716; SERVERID=db2965ac6d1d4f093009cf9d5cafcc6f|1598507749|1598507696
    Host: www.kfc.com.cn
    User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36
    X-Requested-With: XMLHttpRequest
    '''
    headers=headers_raw_to_dict(r_h)
    data_list = []
    base_url='http://www.kfc.com.cn/kfccda/storelist/index.aspx'
    store_url='http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
    keys=['page', 'province','city','name','address','pro']

    def get_city(self):
        try:
            resp=session.get(url=self.base_url,headers=self.headers,verify=False).content.decode()
            sel=Selector(resp)
            all_li=sel.xpath("//ul[@class='shen_info']/li")
            for p in all_li:
                all_a=p.xpath("./div[@class='shen_city']/a")
                for c in all_a:
                    city=c.xpath("./text()").get()
                    form={"cname":city,"pid":"","pageIndex": "1","pageSize": "10"}
                    self.get_store(form)
                    time.sleep(random.random()+1)
        except Exception:
            traceback.print_exc()

    def get_store(self,form):
        try:
            page_resp=session.post(url=self.store_url,headers=self.headers,verify=False,data=form).json()
            if page_resp:
                total=page_resp['Table'][0]['rowcount']
                page_count=math.ceil(total/10)
                for page in range(1,page_count+1):
                    form['pageIndex']=str(page)
                    store_resp=session.post(url=self.store_url,headers=self.headers,verify=False,data=form).json()
                    if store_resp:
                        for i in store_resp['Table1']:
                            result=i['rownum'],i['provinceName'],i['cityName'],i['storeName']+"餐厅",i['addressDetail'],i['pro']
                            self.data_list.append(result)
                            logger.info(result)
        except Exception:
            traceback.print_exc()

    def save_store(self):
        all_data=sorted(list(set(self.data_list)),key=self.data_list.index)
        data = pd.DataFrame(all_data,columns=[h.title() for h in self.keys])
        dirname=time.strftime("%Y%m%d",time.localtime())
        os.makedirs(dirname,exist_ok=True)
        skufile='./{0}/'.format(dirname)+self.sku+dirname
        os.makedirs(skufile,exist_ok=True)
        shutil.copy(self.sku+'.py',skufile+'/'+self.sku+'.py')
        data.to_excel(skufile+'/{0}{1}.xlsx'.format(self.sku,dirname),index=False)
        logger.info("data saved successfully")

    def start(self):
        self.get_city()
        self.save_store()

if __name__=='__main__':
    Spider().start()
```